package util

import (
	"context"
	"fmt"
	"strings"
	"sync"
	"sync/atomic"

	"k8s.io/client-go/util/workqueue"
	"k8s.io/klog/v2"

	"volcano.sh/volcano/pkg/scheduler/api"
)

type PredicateHelper interface {
	PredicateNodes(task *api.TaskInfo, nodes []*api.NodeInfo, fn api.PredicateFn, enableErrorCache bool) ([]*api.NodeInfo, *api.FitErrors)
}

type predicateHelper struct {
	taskPredicateErrorCache map[string]map[string]error
}

// PredicateNodes returns the specified number of nodes that fit a task
func (ph *predicateHelper) PredicateNodes(task *api.TaskInfo, nodes []*api.NodeInfo, fn api.PredicateFn, enableErrorCache bool) ([]*api.NodeInfo, *api.FitErrors) {
	var errorLock sync.RWMutex
	fe := api.NewFitErrors()

	// ç‰›é€¼ï¼ŒallNodes æ˜¯ä¸ªæ•°é‡ï¼Œå†™æˆ numNodes ä¼šæ­»ï¼Ÿ
	allNodes := len(nodes)
	if allNodes == 0 {
		return make([]*api.NodeInfo, 0), fe
	}

	// numNodesToFind è¡¨ç¤ºæœŸæœ›çš„é€šè¿‡ predicate åçš„å€™é€‰èŠ‚ç‚¹çš„æœ€å¤§æ•°é‡ï¼Œå¦‚æœå€™é€‰èŠ‚ç‚¹è¿‡å¤§ï¼Œä¸¢å¼ƒæ‰
	numNodesToFind := CalculateNumOfFeasibleNodesToFind(int32(allNodes))

	// è¿™ä¸ªæ³¨é‡ŠğŸ‚ï¼Œå½“åˆ«äººå°å­¦ç”Ÿå‘¢
	//allocate enough space to avoid growing it
	predicateNodes := make([]*api.NodeInfo, numNodesToFind)

	numFoundNodes := int32(0)
	processedNodes := int32(0)

	taskGroupid := taskGroupID(task)
	nodeErrorCache, taskFailedBefore := ph.taskPredicateErrorCache[taskGroupid]
	if nodeErrorCache == nil {
		nodeErrorCache = map[string]error{}
	}

	//create a context with cancellation
	ctx, cancel := context.WithCancel(context.Background())

	checkNode := func(index int) {
		// Check the nodes starting from where is left off in the previous scheduling cycle,
		// to make sure all nodes have the same chance of being examined across pods.

		// ç‰›é€¼ï¼ŒlastProcessedNodeIndex æ˜¯ä¸ªå…¨å±€å˜é‡ï¼Œä½œè€…çš„æ„æ€åº”è¯¥æ˜¯æƒ³å°½å¯èƒ½åœ°éå†æ‰€æœ‰èŠ‚ç‚¹ï¼Œä¸‹æ¬¡åˆ†é…ä»ä¸Šæ¬¡åˆ†é…ç»“æŸçš„èŠ‚ç‚¹å¼€å§‹
		node := nodes[(lastProcessedNodeIndex+index)%allNodes]
		atomic.AddInt32(&processedNodes, 1)
		klog.V(4).Infof("Considering Task <%v/%v> on node <%v>: <%v> vs. <%v>",
			task.Namespace, task.Name, node.Name, task.Resreq, node.Idle)

		// Check if the task had "predicate" failure before.
		// And then check if the task failed to predict on this node before.
		if enableErrorCache && taskFailedBefore {
			errorLock.RLock()
			errC, ok := nodeErrorCache[node.Name]
			errorLock.RUnlock()

			if ok {
				errorLock.Lock()
				fe.SetNodeError(node.Name, errC)
				errorLock.Unlock()
				return
			}
		}

		// TODO (k82cn): Enable eCache for performance improvement.
		if _, err := fn(task, node); err != nil {
			klog.V(3).Infof("Predicates failed: %v", err)
			errorLock.Lock()
			nodeErrorCache[node.Name] = err
			ph.taskPredicateErrorCache[taskGroupid] = nodeErrorCache
			fe.SetNodeError(node.Name, err)
			errorLock.Unlock()
			return
		}

		//check if the number of found nodes is more than the numNodesTofind
		// è¿™é‡Œæ˜¯åœ¨å¹¶å‘å®‰å…¨çš„å¢åŠ  idxï¼Œå¾ˆè ¢ï¼Œç›´æ¥ mutx+append å°±è¡Œäº†
		length := atomic.AddInt32(&numFoundNodes, 1)
		if length > numNodesToFind {
			cancel()
			atomic.AddInt32(&numFoundNodes, -1) // åˆåŠ åˆå‡çš„ï¼ŒğŸ‚ğŸº
		} else {
			// å¦‚æœæ²¡æœ‰ä»€ä¹ˆç‰¹æ®ŠåŸå› ï¼Œä¸è¦ç”¨ length-1 è¿™ç§ä¸œè¥¿ï¼Œè¯¥æ˜¯å•¥å°±æ˜¯å•¥ï¼Œè¿™é‡Œå°±åº”è¯¥æ˜¯ predicateNodes[i] = node
			predicateNodes[length-1] = node
		}
	}

	//workqueue.ParallelizeUntil(context.TODO(), 16, len(nodes), checkNode)
	workqueue.ParallelizeUntil(ctx, 16, allNodes, checkNode)

	//processedNodes := int(numFoundNodes) + len(filteredNodesStatuses) + len(failedPredicateMap)
	lastProcessedNodeIndex = (lastProcessedNodeIndex + int(processedNodes)) % allNodes
	predicateNodes = predicateNodes[:numFoundNodes] // numFoundNodes å¯èƒ½ä¸è¶³ capacity, å†æˆªæ–­ä¸€ä¸‹
	return predicateNodes, fe
}

func taskGroupID(task *api.TaskInfo) string {
	return fmt.Sprintf("%s/%s", task.Job, task.GetTaskSpecKey())
}

func NewPredicateHelper() PredicateHelper {
	return &predicateHelper{taskPredicateErrorCache: map[string]map[string]error{}}
}

type StatusSets []*api.Status

func (s StatusSets) ContainsUnschedulable() bool {
	for _, status := range s {
		if status == nil {
			continue
		}
		if status.Code == api.Unschedulable {
			return true
		}
	}
	return false
}

func (s StatusSets) ContainsUnschedulableAndUnresolvable() bool {
	for _, status := range s {
		if status == nil {
			continue
		}
		if status.Code == api.UnschedulableAndUnresolvable {
			return true
		}
	}
	return false
}

func (s StatusSets) ContainsErrorSkipOrWait() bool {
	for _, status := range s {
		if status == nil {
			continue
		}
		if status.Code == api.Error || status.Code == api.Skip || status.Code == api.Wait {
			return true
		}
	}
	return false
}

// Message return the message generated from StatusSets
func (s StatusSets) Message() string {
	if s == nil {
		return ""
	}
	all := make([]string, 0, len(s))
	for _, status := range s {
		if status.Reason == "" {
			continue
		}
		all = append(all, status.Reason)
	}
	return strings.Join(all, ",")
}

// Reasons return the reasons list
func (s StatusSets) Reasons() []string {
	if s == nil {
		return nil
	}
	all := make([]string, 0, len(s))
	for _, status := range s {
		if status.Reason == "" {
			continue
		}
		all = append(all, status.Reason)
	}
	return all
}
